# -*- coding: utf-8 -*-
"""Stage_2_EDA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EQ1Kgg4IYnjqL_dRehy-9iKY_jeZjKc1
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime
import wordcloud
import plotly.express as px

import os
from google.colab import drive
drive.mount('/content/drive')

transactions=pd.read_csv('/content/drive/MyDrive/HM_articles_recommendation/transactions_train.csv',dtype={'article_id': str})

art = pd.read_csv('/content/drive/MyDrive/HM_articles_recommendation/articles.csv',dtype={'article_id': str})

customers=pd.read_csv('/content/drive/MyDrive/HM_articles_recommendation/customers.csv')

"""**Transaction Data Analysis**

### Dominant categories
"""

transactions.head()

transactions.describe()

missing_col = transactions.columns[transactions.isna().any()].tolist()
print(f'The columns with missing values are {missing_col}')
#No missing values

article_sales = transactions.groupby(['article_id']).size().reset_index()
Top_sales = article_sales.sort_values(by=0, axis=0, ascending=False)[:15]
Top_sales = pd.merge(Top_sales, art, how='inner', on='article_id')

Top_sales

fig, ax = plt.subplots(1, 1, figsize=(12, 10))
sns.barplot(x=Top_sales['prod_name'], y=Top_sales[0], ax=ax, ci=None)
ax.set_ylabel('Frequency')
ax.set_title('Top 10 Article Sales Count Plot')
ax.set_xticklabels(ax.get_xticklabels(), rotation=45)

Top_sales

transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])
transactions['month'] = pd.DatetimeIndex(transactions['t_dat']).month
date_sales = transactions.groupby(['month']).size().reset_index()
#Top_date = date_sales.sort_values(by=0, axis=0, ascending=False)

date_sales

fig, ax = plt.subplots(1, 1, figsize=(12, 10))
sns.barplot(x=date_sales['month'], y=date_sales[0], ax=ax)
ax.set_xlabel('Month')
ax.set_ylabel('Frequency')
ax.set_title('Monthly Transaction Frequency Plot')

"""**Articles Data Analysis**

### Dominant categories
"""

art.head()

"""- `product_code` **108775**: Strap top
    - `article_id`: **108775**_051: with stripe; off white
    - `article_id`: **108775**_044: solid; white
    - ...
- Each `product_code` corresponds to multiple `article_id`; the later represents different colors, style, etc. of the product
"""

art.isna().mean().sort_values(ascending=False)

"""- The column `detail_desc` has some missing data (~0.4%). No other columns have any missing value.
- The number of missing data is very small, and there's no easy fix for missing text data.
- For models that use NLP on this column, we'll simply drop those rows (and the corresponding transactions). For models that don't rely on this column, there's no missing data.
"""

sns.ecdfplot(data=art.groupby("product_code", as_index=False).size(), x="size", log_scale=True)
plt.xlabel("Number of options for the product")
plt.ylabel("Empirical CDF")
plt.xticks([1, 2, 3, 5, 10, 20, 50, 70], [1, 2, 3, 5, 10, 20, 50, 70]);

"""- Most products have few options (below 4), while there are products with lots of options, e.g. product 783707 is a kind of socks with 75 options.
- The `transactions` table (each row is `(customer_id, article_id)`) is very sparse in terms of `article_id`. Replacing `article_id` with `product_id` may help during training.
"""

print("Product attributes and their unique values:")
attr = ['index_group_name', 'perceived_colour_value_name', 'index_name',
        'product_group_name', 'perceived_colour_master_name',
        'garment_group_name', 'graphical_appearance_name',
        'colour_group_name', 'section_name',
        'product_type_name', 'department_name']
{a: len(set(art[a])) for a in attr}

"""- `index_group_name`, `index_name`, `section_name`, and `department_name` are hierarchical
- `product_group_name` and `product_type_name` are also hierarchical.
"""

print("Count of article_id by several attributes.")

fig, ax = plt.subplots(4, 1, figsize=(4, 16))

cat_var = ["index_name", "perceived_colour_master_name", "garment_group_name", "graphical_appearance_name"]
for i in range(4):
    sns.countplot(data=art, y=cat_var[i],
                  color="blue", ax=ax[i],
                  order=art[cat_var[i]].value_counts().index[:20],
                  )

print("Most frequent words in product descriptions in each category (index_name)")
for name, df in art.groupby("index_name"):
    text = " ".join(set(df.detail_desc.apply(str)))
    wc = wordcloud.WordCloud().generate(text)
    plt.imshow(wc)
    plt.axis("off")
    plt.title(name)
    plt.show()

df_trans = transactions
df_trans['t_dat'] = pd.to_datetime(df_trans['t_dat'])
df_trans = df_trans[df_trans['t_dat'] >= pd.to_datetime('2020-07-01')]
df_article =  art
df_article['idxgrp_idx_prdtyp'] = df_article['index_group_name'] + '_' + df_article['index_name'] + '_' + df_article['product_type_name']

# Merge the two dataframed
df = pd.merge(
    df_trans,
    df_article,
    on='article_id',
    how='left'
)

df['product_code'] = df['product_code'].astype(str)
df['num_week'] = df['t_dat'].dt.isocalendar().week
df['product_code'] = df['product_code'].astype(str)

"""### Purchasing pattern prediction

a. Compare a set of products purchased in any given week with a set of products purchased 1, 2, or 3 weeks ago to see if they contain the SAME products.
"""

def same_pur(df, agg_key):
    dfagg = df.groupby(['num_week','customer_id'])[[agg_key]].agg({
            agg_key: lambda x: ','.join(x)
    }).reset_index().rename(columns={agg_key: 'purchased_set'})
    dfagg['num_2wk_before'] = dfagg['num_week'] + 2
    dfagg = pd.merge(
        dfagg[['num_week','customer_id','purchased_set']],
        dfagg.rename(columns={'purchased_set': '2wk_before_purchased_set'})[['num_2wk_before','customer_id','2wk_before_purchased_set']],
        left_on=['num_week', 'customer_id'],
        right_on=['num_2wk_before', 'customer_id'],
        how='left'
    )
    dfagg['num_1wk_before'] = dfagg['num_week'] + 1
    dfagg = pd.merge(
        dfagg,
        dfagg.rename(columns={'purchased_set': '1wk_before_purchased_set'})[['num_1wk_before','customer_id','1wk_before_purchased_set']],
        left_on=['num_week', 'customer_id'],
        right_on=['num_1wk_before', 'customer_id'],
        how='left'
    )
    dfagg['num_3wk_before'] = dfagg['num_week'] + 3
    dfagg = pd.merge(
        dfagg,
        dfagg.rename(columns={'purchased_set': '3wk_before_purchased_set'})[['num_3wk_before','customer_id','3wk_before_purchased_set']],
        left_on=['num_week', 'customer_id'],
        right_on=['num_3wk_before', 'customer_id'],
        how='left'

    )
    dfagg = dfagg[['num_week','customer_id','purchased_set','1wk_before_purchased_set','2wk_before_purchased_set','3wk_before_purchased_set']]
    for col in ['purchased_set','1wk_before_purchased_set', '2wk_before_purchased_set', '3wk_before_purchased_set']:
        dfagg[col] = dfagg[col].fillna('')
        dfagg[col] = dfagg[col].str.split(',')
    dfagg['2wk_before_purchased_set'] = dfagg['2wk_before_purchased_set'] + dfagg['1wk_before_purchased_set']
    dfagg['3wk_before_purchased_set'] = dfagg['3wk_before_purchased_set'] + dfagg['2wk_before_purchased_set']
    for col in ['purchased_set','1wk_before_purchased_set', '2wk_before_purchased_set', '3wk_before_purchased_set']:
        dfagg[col] = dfagg[col].map(set)

    dfagg['is_purchased_same_within_1wk'] = (dfagg['purchased_set'] & dfagg['1wk_before_purchased_set']).astype(int)
    dfagg['is_purchased_same_within_2wk'] = (dfagg['purchased_set'] & dfagg['2wk_before_purchased_set']).astype(int)
    dfagg['is_purchased_same_within_3wk'] = (dfagg['purchased_set'] & dfagg['3wk_before_purchased_set']).astype(int)
    print(
        len(dfagg[dfagg['is_purchased_same_within_3wk'] == 1]['customer_id'].unique()) / len(dfagg['customer_id'].unique()) * 100,
        len(dfagg[dfagg['is_purchased_same_within_2wk'] == 1]['customer_id'].unique()) / len(dfagg['customer_id'].unique()) * 100,
        len(dfagg[dfagg['is_purchased_same_within_1wk'] == 1]['customer_id'].unique()) / len(dfagg['customer_id'].unique()) * 100
    )
    df_vis = pd.DataFrame({
        'Pediod': ['Within_1wk', 'Within_2wk', 'Within_3wk'],
        'Ratio': [len(dfagg[dfagg['is_purchased_same_within_1wk'] == 1]['customer_id'].unique()) / len(dfagg['customer_id'].unique()) * 100,
                  len(dfagg[dfagg['is_purchased_same_within_2wk'] == 1]['customer_id'].unique()) / len(dfagg['customer_id'].unique()) * 100,
                  len(dfagg[dfagg['is_purchased_same_within_3wk'] == 1]['customer_id'].unique()) / len(dfagg['customer_id'].unique()) * 100]
    })
    fig = px.bar(df_vis, x='Pediod', y='Ratio')
    fig.show()
    return dfagg

dfagg_article = same_pur(df, 'article_id')

"""**Inference**:

- **5.1%** of customers will buy the same product in one week   
- **6.2%** will buy the same product within two weeks  
- **6.6%** will buy the same product within three weeks

b. Do customers buy different colors and sizes of the same product?
"""

dfagg_prdcd = same_pur(df, 'product_code')

"""**Inference**:

- **6.8%** of customers will buy the same product for different color/size in one week   
- **8.5%** will buy the same product for different color/size within two weeks  
- **9.4%** will buy the same product for different color/size within three weeks

**Customers Data Analysis**

### multicolinearity detection
"""

customers = customers.fillna(0)

customers.head()

customers['cms_encode'] = pd.factorize(customers.iloc[:, 3])[0]

customers['fnf_encode'] = pd.factorize(customers.iloc[:, 4])[0]

focus = customers[['FN', 'Active', 'cms_encode', 'fnf_encode', 'age']]
focus.head()

sns.color_palette("flare", as_cmap=True)
sns.heatmap(focus.corr(), cmap="flare")

customers['age']

sns.histplot(data=customers['age'], bins =20)

sns.histplot(data=customers['age'], bins =10)

sns.histplot(data=customers['age'], bins =5)

customer_use = customers[['customer_id', 'cms_encode', 'fnf_encode', 'age']]

#customer_use.to_csv(r'/content/drive/My Drive/HM_articles_recommendation/customer_preprocess.csv')

